import os
import pathlib
from pathlib import Path


import os, sys, threading
import os.path as path
from pathlib import Path, PurePath
import re
from platform import system

import inspect

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf

from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import layers
from tensorflow.keras import models
# from IPython import display


# Set seed for experiment reproducibility
AUTOTUNE = tf.data.experimental.AUTOTUNE
EPOCHS = 10
seed = 42
tf.random.set_seed(seed)
np.random.seed(seed)
ROOT_DIR = None
def setup():
    main_id = None
    for t in threading.enumerate():
        if t.name == 'MainThread':
            main_id = t.ident
            break

    if not main_id:
        raise RuntimeError("Main thread exited before execution")

    current_main_frame = sys._current_frames()[main_id]
    base_frame = inspect.getouterframes(current_main_frame)[-1]

    if system().lower() == 'windows':
        filename = base_frame.filename
    else:
        filename = base_frame[0].f_code.co_filename

    global ROOT_DIR
    ROOT_DIR = os.path.dirname(os.path.abspath(filename))
    return ROOT_DIR


# if not data_dir.exists():
#   tf.keras.utils.get_file(
#       'mini_speech_commands.zip',
#       origin="http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip",
#       extract=True,
#       cache_dir='.', cache_subdir='data')



def decode_audio(audio_binary):
  audio, _ = tf.audio.decode_wav(audio_binary,desired_samples=16000)
  print(f"Audio shape: {audio.shape}\n")
  return tf.squeeze(audio, axis=-1)


def get_label(file_path):
  parts = tf.strings.split(file_path, os.path.sep)

  # Note: You'll use indexing here instead of tuple unpacking to enable this 
  # to work in a TensorFlow graph.
  return parts[-2]

def get_waveform_and_label(file_path):
  label = get_label(file_path)
  audio_binary = tf.io.read_file(file_path)
  waveform = decode_audio(audio_binary)
  return waveform, label


# rows = 3
# cols = 3
# n = rows*cols
# fig, axes = plt.subplots(rows, cols, figsize=(10, 12))
# for i, (audio, label) in enumerate(waveform_ds.take(n)):
#   r = i // cols
#   c = i % cols
#   ax = axes[r][c]
#   ax.plot(audio.numpy())
#   ax.set_yticks(np.arange(-1.2, 1.2, 0.2))
#   label = label.numpy().decode('utf-8')
#   ax.set_title(label)

# plt.show()

def commands():
    chords_path = Path(setup())
    chords_path = chords_path / 'Guitar_Chords'
    data_dir = pathlib.Path(chords_path)
    commands = np.array(tf.io.gfile.listdir(str(data_dir)))
    commands = commands[commands != 'README.md']
    return commands

def get_spectrogram(waveform):
  # Padding for files with less than 16000 samples
  zero_padding = tf.zeros([16000] - tf.shape(waveform), dtype=tf.float32)

  # Concatenate audio with padding so that all audio clips will be of the 
  # same length
  waveform = tf.cast(waveform, tf.float32)
  equal_length = tf.concat([waveform, zero_padding], 0)
  spectrogram = tf.signal.stft(
      equal_length, frame_length=255, frame_step=128)
      
  spectrogram = tf.abs(spectrogram)

  return spectrogram

# for waveform, label in waveform_ds.take(1):
#   label = label.numpy().decode('utf-8')
#   spectrogram = get_spectrogram(waveform)

# print('Label:', label)
# print('Waveform shape:', waveform.shape)
# print('Spectrogram shape:', spectrogram.shape)
# print('Audio playback')

def plot_spectrogram(spectrogram, ax):
  # Convert to frequencies to log scale and transpose so that the time is
  # represented in the x-axis (columns).
  log_spec = np.log(spectrogram.T)
  height = log_spec.shape[0]
  X = np.arange(16000, step=height + 1)
  Y = range(height)
  ax.pcolormesh(X, Y, log_spec)

# fig, axes = plt.subplots(2, figsize=(12, 8))
# timescale = np.arange(waveform.shape[0])
# axes[0].plot(timescale, waveform.numpy())
# axes[0].set_title('Waveform')
# axes[0].set_xlim([0, 16000])
# plot_spectrogram(spectrogram.numpy(), axes[1])
# axes[1].set_title('Spectrogram')
# plt.show()

def get_spectrogram_and_label_id(audio, label,commands=commands()):
  spectrogram = get_spectrogram(audio)
  spectrogram = tf.expand_dims(spectrogram, -1)
  label_id = tf.argmax(label == commands)
  return spectrogram, label_id



# rows = 3
# cols = 3
# n = rows*cols
# fig, axes = plt.subplots(rows, cols, figsize=(10, 10))
# for i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):
#   r = i // cols
#   c = i % cols
#   ax = axes[r][c]
#   plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)
#   ax.set_title(commands[label_id.numpy()])
#   ax.axis('off')
  
# plt.show()

def preprocess_dataset(files):
  files_ds = tf.data.Dataset.from_tensor_slices(files)
  output_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)
  output_ds = output_ds.map(
      get_spectrogram_and_label_id,  num_parallel_calls=AUTOTUNE)
  return output_ds



def chord_classifier_model(norm_layer,input_shape,num_labels):
    model = models.Sequential([
        layers.Input(shape=input_shape),
        preprocessing.Resizing(32, 32), 
        norm_layer,
        layers.Conv2D(32, 3, activation='relu'),
        layers.Conv2D(64, 3, activation='relu'),
        layers.MaxPooling2D(),
        layers.Dropout(0.25),
        layers.Flatten(),
        layers.Dense(128, activation='relu'),
        layers.Dropout(0.5),
        layers.Dense(num_labels),
    ])

    # model.summary()

    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'],
    )
    return model



def train_model(model, train_ds,val_ds):
    global EPOCHS
    history = model.fit(
        train_ds, 
        validation_data=val_ds,  
        epochs=EPOCHS,
        callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),
    )
    save_model(model)
    return history

def plot_results_training(history):
    metrics = history.history
    plt.plot(history.epoch, metrics['loss'], metrics['accuracy'])
    plt.legend(['loss', 'val_loss'])
    plt.show()

# TODO: TESTING method
# test_audio = []
# test_labels = []

# for audio, label in test_ds:
#   test_audio.append(audio.numpy())
#   test_labels.append(label.numpy())

# test_audio = np.array(test_audio)
# test_labels = np.array(test_labels)

# y_pred = np.argmax(model.predict(test_audio), axis=1)
# y_true = test_labels

# test_acc = sum(y_pred == y_true) / len(y_true)
# print(f'Test set accuracy: {test_acc:.0%}')

# confusion_mtx = tf.math.confusion_matrix(y_true, y_pred) 
# plt.figure(figsize=(10, 8))
# sns.heatmap(confusion_mtx, xticklabels=commands, yticklabels=commands, 
#             annot=True, fmt='g')
# plt.xlabel('Prediction')
# plt.ylabel('Label')
# plt.show()

model_path = Path(setup())
model_path = model_path / 'New_Saved_Model'

# model.save(model_path/'chords_classifier_model')

def continue_training():
    global AUTOTUNE
    chords_path = Path(setup())
    chords_path = chords_path / 'Guitar_Chords'
    data_dir = pathlib.Path(chords_path)
    commands = np.array(tf.io.gfile.listdir(str(data_dir)))
    commands = commands[commands != 'README.md']
    print('Commands:', commands)

    filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')
    filenames = tf.random.shuffle(filenames)
    num_samples = len(filenames)
    # print('Number of total examples:', num_samples)
    # print('Number of examples per label:',
    #     len(tf.io.gfile.listdir(str(data_dir/commands[0]))))
    # print('Example file tensor:', filenames[0])

    train_files = filenames[:6400]
    val_files = filenames[6400: 6400 + 800]
    test_files = filenames[-800:]

    print('Training set size', len(train_files))
    print('Validation set size', len(val_files))
    print('Test set size', len(test_files))
    
    files_ds = tf.data.Dataset.from_tensor_slices(train_files)
    waveform_ds = files_ds.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)
    EPOCHS = 10

    spectrogram_ds = waveform_ds.map(
    get_spectrogram_and_label_id, num_parallel_calls=AUTOTUNE)

    train_ds = spectrogram_ds
    val_ds = preprocess_dataset(val_files)
    test_ds = preprocess_dataset(test_files)

    batch_size = 64
    train_ds = train_ds.batch(batch_size)
    val_ds = val_ds.batch(batch_size)

    for spectrogram, _ in spectrogram_ds.take(1):
        input_shape = spectrogram.shape
    print('Input shape:', input_shape)
    num_labels = len(commands)

    norm_layer = preprocessing.Normalization()
    norm_layer.adapt(spectrogram_ds.map(lambda x, _: x))

    train, val, test = train_ds, val_ds, test_ds#divide_ds(files=chords_files())
    # train, val, test = filenames_to_tensor_slices(train),filenames_to_tensor_slices(val),filenames_to_tensor_slices(test),
    # train_ds = train.map(get_waveform_and_label, num_parallel_calls=AUTOTUNE)
    # train_ds = train_ds.map(get_spectrogram_and_label_id,num_parallel_calls=AUTOTUNE)
    # val_ds = preprocess_dataset(val)
    # test_ds = preprocess_dataset(test)

    # in_shape = None
    # for  spectrogram, _ in train.take(1):
    #     in_shape = spectrogram.shape
    model = chord_classifier_model(norm_layer,input_shape=input_shape,num_labels=num_labels)
    model = load_model(model)
    hist = train_model(train_ds=train, val_ds=val,model=model )
    save_model(model)
    # plot_results_training(hist)
    return

def save_model(model):
    model.save_weights(model_path/'./checkpoints/latest-checkpoint')
    model.save(model_path/'chords_classifier_model')
    return

def load_model(model):
    model.load_weights(model_path / './checkpoints/latest-checkpoint')
    return model
    # plot_model_loss(hist)
# sample_file = data_dir/'a/a.wav'

# sample_ds = preprocess_dataset([str(sample_file)])

# for spectrogram, label in sample_ds.batch(1):
#   prediction = model(spectrogram)
#   plt.bar(commands, tf.nn.softmax(prediction[0]))
#   plt.title(f'Predictions for "{commands[label[0]]}"')
#   plt.show()

if __name__ == '__main__':
    continue_training()